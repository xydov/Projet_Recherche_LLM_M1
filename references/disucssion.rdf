<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/">
    <bib:Article rdf:about="https://proceedings.neurips.cc/paper_files/paper/2023/hash/5d570ed1708bbe19cb60f7a7aff60575-Abstract-Conference.html">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>36</prism:volume>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tong</foaf:surname>
                        <foaf:givenName>Shengbang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jones</foaf:surname>
                        <foaf:givenName>Erik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steinhardt</foaf:surname>
                        <foaf:givenName>Jacob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_361"/>
        <dcterms:isReferencedBy rdf:resource="#item_379"/>
        <link:link rdf:resource="#item_243"/>
        <link:link rdf:resource="#item_362"/>
        <dc:title>Mass-producing failures of multimodal systems with language models</dc:title>
        <dc:date>2024</dc:date>
        <z:libraryCatalog>Google Scholar</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper_files/paper/2023/hash/5d570ed1708bbe19cb60f7a7aff60575-Abstract-Conference.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-26 17:38:34</dcterms:dateSubmitted>
    </bib:Article>
   <bib:Memo rdf:about="#item_361"><rdf:value></rdf:value></bib:Memo>
    <bib:Memo rdf:about="#item_379">
        <rdf:value>&lt;div data-schema-version=&quot;8&quot;&gt;&lt;p&gt;Devant les ́echecs r ́ecurrents des syst`emes multimodaux bas ́es sur le texte&lt;br&gt;lors de leur mise en service, MULTIMON ́emerge comme une avanc ́ee significa-&lt;br&gt;tive. Ces syst`emes, destin ́es `a transmuter le texte en images, en sc`enes 3D,&lt;br&gt;ou en vid ́eos, font face `a de multiples obstacles en conditions r ́eelles, malgr ́e&lt;br&gt;des essais exhaustifs. MULTIMON se pr ́esente comme une solution novatrice&lt;br&gt;`a cette probl ́ematique, agissant en tant que syst`eme d’ ́evaluation syst ́ematique&lt;br&gt;et adapt ́e aux exigences humaines. Il d ́ebusque les d ́efaillances syst ́emiques en&lt;br&gt;exploitant les erreurs d’accord et en se servant de mod`eles de langage tels que&lt;br&gt;GPT-4 pour ́elaborer des explications compr ́ehensibles, ce qui s’av`ere pr ́ecieux&lt;br&gt;pour les concepteurs de syst`emes&lt;br&gt;L’impact de MULTIMON sur le d ́eveloppement des mod`eles neuronaux de&lt;br&gt;langage multimodaux est remarquable, proposant une m ́ethode ́evolutible pour&lt;br&gt;d ́etecter les d ́efaillances syst ́ematiques tˆot et am ́eliorer ainsi la fiabilit ́e et la&lt;br&gt;s ́ecurit ́e avant leur d ́eploiement. Cependant, l’utilisation de MULTIMON r ́ev`ele&lt;br&gt; ́egalement les risques li ́es `a l’utilisation r ́ep ́et ́ee de composants comme CLIP dans&lt;br&gt;diff ́erents syst`emes, en montrant comment les d ́efaillances peuvent se propager&lt;br&gt;d’un syst`eme `a l’autre. MULTIMON se distingue par sa capacit ́e `a d ́ecouvrir et&lt;br&gt;cat ́egoriser de nouvelles d ́efaillances grˆace `a l’utilisation de mod`eles de langage,&lt;br&gt;ouvrant la voie `a des applications IA plus robustes et align ́ees sur les com-&lt;br&gt;plexit ́es du monde r ́eel. Cette innovation sugg`ere un avenir o`u les mod`eles de&lt;br&gt;langage transcendent leur rˆole traditionnel de g ́en ́erateurs de texte pour devenir&lt;br&gt;de v ́eritables bases de connaissances dynamiques.&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_243">
        <z:itemType>attachment</z:itemType>
        <dc:title>Available Version (via Google Scholar)</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper_files/paper/2023/file/5d570ed1708bbe19cb60f7a7aff60575-Paper-Conference.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-26 17:39:39</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_362">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/362/Tong et al. - 2024 - Mass-producing failures of multimodal systems with.pdf"/>
        <dc:title>Texte intégral</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper_files/paper/2023/file/5d570ed1708bbe19cb60f7a7aff60575-Paper-Conference.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-03-24 15:41:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2307.02469">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeng</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hanbo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Jiani</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xia</foaf:surname>
                        <foaf:givenName>Jiangnan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>Guoqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Yuchen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kong</foaf:surname>
                        <foaf:givenName>Tao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_380"/>
        <link:link rdf:resource="#item_257"/>
        <link:link rdf:resource="#item_258"/>
        <link:link rdf:resource="#item_365"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Computation and Language</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</dc:title>
        <dcterms:abstract>Recent advancements in Large Language Models (LLMs) such as GPT4 have displayed exceptional multi-modal capabilities in following open-ended instructions given images. However, the performance of these models heavily relies on design choices such as network structures, training data, and training strategies, and these choices have not been extensively discussed in the literature, making it difficult to quantify progress in this field. To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models. We implement over 20 variants with controlled settings. Concretely, for network structures, we compare different LLM backbones and model designs. For training data, we investigate the impact of data and sampling strategies. For instructions, we explore the influence of diversified prompts on the instruction-following ability of the trained models. For benchmarks, we contribute the first, to our best knowledge, comprehensive evaluation set including both image and video tasks through crowd-sourcing. Based on our findings, we present Lynx, which performs the most accurate multi-modal understanding while keeping the best multi-modal generation ability compared to existing open-sourced GPT4-style models.</dcterms:abstract>
        <dc:date>2023-07-30</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2307.02469</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-28 07:34:30</dcterms:dateSubmitted>
        <dc:description>Issue: arXiv:2307.02469
Issue: arXiv:2307.02469
arXiv:2307.02469 [cs]
version: 2</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2307.02469</dc:identifier>
        <prism:number>arXiv:2307.02469</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_380">
        <rdf:value>&lt;div data-schema-version=&quot;8&quot;&gt;&lt;p&gt;Cette ́etude se penche sur les choix de conception essentiels pour entraˆıner&lt;br&gt;des mod`eles de langage `a la mani`ere de GPT-4, utilisant des entr ́ees multi-&lt;br&gt;modales. Elle fournit une analyse `a la fois quantitative et qualitative sur la&lt;br&gt;formation de ces mod`eles, examinant les fondements des mod`eles de langage&lt;br&gt;`a grande ́echelle, l’impact des strat ́egies de donn ́ees et d’ ́echantillonnage, et&lt;br&gt;l’influence de prompts vari ́es. Un mod`ele nomm ́e LYNX est introduit, se dis-&lt;br&gt;tinguant par sa compr ́ehension multimodale pr ́ecise et sa capacit ́e robuste de&lt;br&gt;g ́en ́eration multimodale, ́evalu ́e sur divers benchmarks pour approfondir notre&lt;br&gt;compr ́ehension de ses performances dans des tˆaches de perception, cognition et&lt;br&gt;action.&lt;br&gt;Les r ́esultats des exp ́eriences men ́ees avec LYNX offrent des aper ̧cus pr ́ecieux&lt;br&gt;pour la recherche future sur les mod`eles de langage multimodaux, influen ̧cant&lt;br&gt;potentiellement les choix architecturaux et les strat ́egies d’entraˆınement. Cepen-&lt;br&gt;dant, l’ ́etude reconnaˆıt certaines limites, notamment dans les m ́etriques d’ ́evaluation&lt;br&gt;pour les mod`eles multimodaux et les d ́efis de trouver la combinaison de donn ́ees&lt;br&gt;optimale pour l’entraˆınement. Malgr ́e ces obstacles, LYNX surpasse les mod`eles&lt;br&gt;existants en compr ́ehension et g ́en ́eration multimodales, mettant en lumi`ere&lt;br&gt;l’importance d’ ́equilibrer correctement ces capacit ́es. Cette recherche illus-&lt;br&gt;tre l’importance de parvenir `a un ́equilibre entre l’interpr ́etation de donn ́ees&lt;br&gt;vari ́ees et la production de textes significatifs. Elle pr ́epare le terrain pour le&lt;br&gt;d ́eveloppement futur de mod`eles linguistiques capables de naviguer `a travers un&lt;br&gt; ́eventail d’informations tout en capturant l’essence et la finesse de la communi-&lt;br&gt;cation humaine&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_257">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2307.02469v2.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-28 07:34:49</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_258">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2307.02469</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-28 07:34:56</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_365">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/365/Zeng et al. - 2023 - What Matters in Training a GPT4-Style Language Mod.pdf"/>
        <dc:title>Texte intégral</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2307.02469.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-03-24 15:43:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
</rdf:RDF>
